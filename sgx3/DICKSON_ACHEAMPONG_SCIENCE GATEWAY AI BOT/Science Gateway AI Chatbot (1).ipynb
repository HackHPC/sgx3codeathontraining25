{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec8e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully scraped 636 entries and saved to 'science_gateways_extended.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_science_gateways():\n",
    "    base_url = \"https://sciencegateways.org\"\n",
    "    start_url = f\"{base_url}/resources/browse?search=&sortby=date&tag=&type=&limit=1000&limitstart=0\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(start_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching main page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    gateways = []\n",
    "\n",
    "    for item in soup.select('li.public'):\n",
    "        name = item.select_one('p.title')\n",
    "        category = item.select_one('p.details')\n",
    "        abstract = item.select_one('p.result-description')\n",
    "        detail_tag = item.select_one('p.title a')\n",
    "\n",
    "        # Extract text if element exists, else None\n",
    "        name_text = name.get_text(strip=True) if name else None\n",
    "        category_text = category.get_text(strip=True) if category else None\n",
    "        abstract_text = abstract.get_text(strip=True) if abstract else None\n",
    "        detail_link = f\"{base_url}{detail_tag['href']}\" if detail_tag and detail_tag.has_attr('href') else None\n",
    "        \n",
    "        # Scrape additional details if link exists\n",
    "        additional_data = scrape_additional_details(detail_link, headers) if detail_link else {}\n",
    "\n",
    "        gateways.append({\n",
    "            \"name\": name_text,\n",
    "            \"category\": category_text,\n",
    "            \"abstract\": abstract_text,\n",
    "            \"site\": detail_link,\n",
    "            **additional_data,\n",
    "            \"date_scraped\": datetime.utcnow().isoformat()  # Timestamp for tracking\n",
    "        })\n",
    "\n",
    "    with open(\"science_gateways_extended.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gateways, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Successfully scraped {len(gateways)} entries and saved to 'science_gateways_extended.json'.\")\n",
    "\n",
    "def scrape_additional_details(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching details from {url}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the 'Published on' date\n",
    "    published_tag = soup.select_one(\"div.resource-metadata p\")\n",
    "    published_date = published_tag.get_text(strip=True).replace(\"Published on:\", \"\").strip() if published_tag else None\n",
    "\n",
    "    # Extract external site URL and ensure it's a full link\n",
    "    site_tag = soup.select_one(\"div.resource-content a[href]\")\n",
    "    site_url = site_tag['href'] if site_tag and site_tag['href'].startswith(\"http\") else None\n",
    "\n",
    "    # Extract citation text\n",
    "    cite_tag = soup.select_one(\"ul.citations p\")\n",
    "    cite_text = cite_tag.get_text(strip=True) if cite_tag else None\n",
    "\n",
    "    # Extract tags\n",
    "    tags_section = soup.select(\".tags a\")\n",
    "    tags = [tag.get_text(strip=True) for tag in tags_section] if tags_section else None\n",
    "\n",
    "    return {\n",
    "        \"published_on\": published_date,\n",
    "        \"external_site\": site_url,\n",
    "        \"citation\": cite_text,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "scrape_science_gateways()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06659d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Welcome to the Science Gateway AI Chatbot! Type 'exit' to quit.\n",
      "\n",
      "üìù Type your message OR üéô Say 'voice' to speak: What are some cool science gateways?\n",
      "\n",
      "ü§ñ **Science Gateway AI**: Here are some cool science gateways that you may find interesting:\n",
      "\n",
      "1. **dREG gateway**\n",
      "   - **Category**: Science Gateways\n",
      "   - **Abstract**: The dREG gateway enables users to identify the location of promoters and enhancers using PRO-seq, GRO-seq, or ChRO-seq data, focusing on transcriptional regulatory elements (TREs) in genomes.\n",
      "   - **Website**: [dREG gateway](https://sciencegateways.org/resources/9610)\n",
      "\n",
      "2. **GeoGateway**\n",
      "   - **Category**: Science Gateways\n",
      "   - **Abstract**: GeoGateway is a data product search and analysis gateway that facilitates scientific discovery, field use, and disaster response by integrating NASA geodetic imaging products with earthquake-related datasets and models.\n",
      "   - **Website**: [GeoGateway](https://sciencegateways.org/resources/9717)\n",
      "\n",
      "3. **COSMIC2 Science Gateway**\n",
      "   - **Category**: Science Gateways\n",
      "   - **Abstract**: The COSMIC2 Science Gateway focuses on structural biology advancements, particularly in cryo-electron microscopy (cryo-EM), enabling the determination of atomic structures of proteins and macromolecular samples.\n",
      "   - **Website**: [COSMIC2\n",
      "\n",
      "\n",
      "üìù Type your message OR üéô Say 'voice' to speak: Show me a list of science gateways related to polar science\n",
      "\n",
      "ü§ñ **Science Gateway AI**: Here are some science gateways related to polar science that you might find interesting:\n",
      "\n",
      "1. **Arctic Data Center**\n",
      "   - **Category**: Science Gateways\n",
      "   - **Abstract**: The Arctic Data Center serves as a repository for data generated from research funded by the National Science Foundation's Arctic Sciences Section. The gateway provides data discovery, access, and user support for Arctic research data.\n",
      "   - **Website**: [Arctic Data Center](https://www.arcticdata.io/)\n",
      "\n",
      "2. **Antarctic Glaciological Data Center (AGDC)**\n",
      "   - **Category**: Science Gateways\n",
      "   - **Abstract**: The Antarctic Glaciological Data Center (AGDC) is a gateway that provides access to data related to Antarctic glaciology, including ice sheet mass balance, glacier dynamics, and ice core records. The gateway aims to facilitate research and collaboration in Antarctic science.\n",
      "   - **Website**: [Antarctic Glaciological Data Center (AGDC)](https://www2.umaine.edu/antarctic-data/)\n",
      "\n",
      "3. **PolarHub: Polar Earth Observing network Gateway**\n",
      "   - **Category**: Science Gateways\n",
      "   - **Abstract**: PolarHub is a gateway that serves as a portal\n",
      "\n",
      "\n",
      "üìù Type your message OR üéô Say 'voice' to speak: exit\n",
      "üëã Goodbye! See you next time!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import speech_recognition as sr\n",
    "from rapidfuzz import process\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Science Gateways data\n",
    "try:\n",
    "    with open(\"science_gateways_optimized.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        gateways = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: The data file 'science_gateways_optimized.json' was not found.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ùå Error: The data file is corrupted or not in JSON format.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "  api_key=\"\"\n",
    ")  # Read API key from environment\n",
    "\n",
    "# Function to search Science Gateway dataset\n",
    "def search_gateways(query):\n",
    "    if not query:\n",
    "        return []\n",
    "\n",
    "    query = query.lower().strip()\n",
    "\n",
    "    # Extract fields for fuzzy matching\n",
    "    names = [(g[\"name\"], i) for i, g in enumerate(gateways) if g.get(\"name\")]\n",
    "    categories = [(g[\"category\"], i) for i, g in enumerate(gateways) if g.get(\"category\")]\n",
    "    abstracts = [(g[\"abstract\"], i) for i, g in enumerate(gateways) if g.get(\"abstract\")]\n",
    "\n",
    "    # Find best matches using rapidfuzz\n",
    "    best_name_match = process.extract(query, [n[0] for n in names], limit=5)\n",
    "    best_category_match = process.extract(query, [c[0] for c in categories], limit=5)\n",
    "    best_abstract_match = process.extract(query, [a[0] for a in abstracts], limit=5)\n",
    "\n",
    "    results = []\n",
    "    for match in best_name_match + best_category_match + best_abstract_match:\n",
    "        if isinstance(match, tuple) and len(match) == 3:\n",
    "            match_text, score, index = match\n",
    "            if score > 60:\n",
    "                results.append(gateways[index])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to interact with GPT-3.5 Turbo (enhanced with Science Gateway data)\n",
    "def gpt_response(user_message, conversation_history):\n",
    "    # Step 1: Try to find a relevant match in the Science Gateway dataset\n",
    "    matches = search_gateways(user_message)\n",
    "\n",
    "    if matches:\n",
    "        relevant_info = \"\\n\".join([\n",
    "            f\"üîπ **{g['name']}**\\nüè∑ **Category**: {g.get('category', 'N/A')}\\nüìÖ **Published On**: {g.get('published_on', 'Unknown')}\\nüìù **Abstract**: {g.get('abstract', 'N/A')[:300]}...\\nüåç **Website**: {g.get('site', 'N/A')}\\n\"\n",
    "            for g in matches[:3]  # Include top 3 relevant results\n",
    "        ])\n",
    "        system_instruction = \"Use the following science gateway information to respond to the user‚Äôs question.\"\n",
    "        prompt = f\"{system_instruction}\\n\\n{relevant_info}\\n\\nUser query: {user_message}\"\n",
    "    else:\n",
    "        prompt = f\"User query: {user_message}\"\n",
    "        print(\"ü§ñ No matching Science Gateways found. I'll provide a general response.\")\n",
    "\n",
    "    # Step 2: Format conversation history for context-aware responses\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a knowledgeable AI assistant specialized in science research tools and resources.\"}\n",
    "    ]\n",
    "    messages.extend(conversation_history)  # Include past messages\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})  # Add current user query\n",
    "\n",
    "    # Step 3: Query OpenAI GPT-3.5 Turbo\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=250\n",
    "        )\n",
    "        bot_response = response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        bot_response = f\"Sorry, an error occurred: {str(e)}\"\n",
    "    \n",
    "    # Step 4: Store the conversation context\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "\n",
    "    return bot_response\n",
    "\n",
    "# Function to get voice input with retries\n",
    "def get_voice_input(max_retries=3):\n",
    "    recognizer = sr.Recognizer()\n",
    "    for attempt in range(max_retries):\n",
    "        with sr.Microphone() as mic:\n",
    "            print(\"\\nüéô Speak now...\")\n",
    "            recognizer.adjust_for_ambient_noise(mic)\n",
    "            try:\n",
    "                audio = recognizer.listen(mic, timeout=8)\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                print(f\"üó£ You said: {text}\\n\")\n",
    "                return text.lower()\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"‚ùå Sorry, I couldn't understand. Please try again.\")\n",
    "            except sr.RequestError:\n",
    "                print(\"‚ö†Ô∏è Error connecting to the speech service. Try again later.\")\n",
    "    print(\"‚ùå Max retries reached. Switching to text input.\")\n",
    "    return None\n",
    "\n",
    "# Start chatbot\n",
    "conversation_history = []  # Store past messages for context-aware responses\n",
    "\n",
    "print(\"ü§ñ Welcome to the Science Gateway AI Chatbot! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"\\nüìù Type your message OR üéô Say 'voice' to speak: \").strip()\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"üëã Goodbye! See you next time!\")\n",
    "        break\n",
    "\n",
    "    if user_input.lower() == \"voice\":\n",
    "        user_query = get_voice_input()\n",
    "    else:\n",
    "        user_query = user_input  # Accept the text input directly\n",
    "\n",
    "    if not user_query:\n",
    "        continue\n",
    "\n",
    "    # Get AI response (uses Science Gateway data when relevant)\n",
    "    bot_response = gpt_response(user_query, conversation_history)\n",
    "    \n",
    "    print(f\"\\nü§ñ **Science Gateway AI**: {bot_response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971c352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88458a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c7070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
